% This document, similar to Estimation.Rnw, takes into account the remarks of advisors given during the meeting of the 6th of December 2021.

\documentclass[a4paper,oneside,12pt,titlepage]{article}

\usepackage{booktabs}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[version=4]{mhchem}
\usepackage[scientific-notation=true]{siunitx}
\usepackage{gensymb}
\usepackage[top=2.5cm, bottom=3cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{float}
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\title{Preliminary Outlier Detection in Missing Values Free data}
\begin{document}
\maketitle
Problems : Valid estimation of a composition of rocks.
\begin{enumerate}
\item composition of major elements with outliers
\item composition of major elements with outliers and missing values
\item : composition of major elements with outliers and missing values
\item : composition of all elements with outliers and missing values
\end{enumerate}

An initial goal of this work is to provide robust estimates of the composixtion of each rock. This problem is made difficult by the presence of both missing values (laboratories did not measure any composition) and outliers (the measures reported are extreme).

Setting aside the problem of missing values for now, one may look at a subcomposition $\mathbf{x} \in \mathbb{S}^D$ of oxides of major elements.

One begin by importing a dataset, for example GeoPT48, a monzonite.


<<echo=FALSE, cache=FALSE, results = FALSE,include = FALSE>>=
library(tidyverse)
library(readr)
library(ggtern)
library(patchwork)
library(compositions)
library(robCompositions)
library(ggfortify)
library(ggrepel)
library(robustbase)
library(rrcov)

geomean <- function(x){
  # compute the geometric mean of a vector
  exp(mean(log(x),na.rm=TRUE)) 
}
@


<<echo=TRUE, cache=TRUE>>=
setwd("/home/max/Documents/MStatistics/MA2/Thesis/Repository/")
data <- read_csv("data/raw/GeoPT48 -84Ra.csv")
data2 <- read_csv("data/raw/GeoPT46 -84Ra.csv")
@
Then, one looks at the subcomposition of major elements :
<<>>=
sel <-c("SiO2","TiO2","Al2O3","Fe2O3T","MnO","MgO","CaO","Na2O",
        "K2O","P2O5")

df.majors.raw <- select(data,all_of(sel))
df.majors2.raw <- select(data2,all_of(sel))

# data cleaning we remove all columns filled with NA
df.majors <- df.majors.raw[rowSums(is.na(df.majors.raw)) != ncol(df.majors.raw),]
df.majors2 <- data.frame(clo(df.majors2.raw[rowSums(is.na(df.majors2.raw)) != ncol(df.majors2.raw),]))

# closure operation
df.majors <- data.frame(clo(df.majors))
dim(df.majors.raw) # 97 observations
dim(df.majors) # 86 observations

# remove columns containing even only one missing value
df.majors.nafree <- df.majors.raw[rowSums(is.na(df.majors.raw)) == 0,] # 62 observations
df.majors2.nafree <- df.majors2.raw[rowSums(is.na(df.majors2.raw)) == 0,] # 78 observations


@


Then  missing values are imputed by the column geometric mean
<<>>=
geomean.v <- sapply(rbind(df.majors),geomean)
for (i in 1:ncol(df.majors)){
  df.majors[,i][is.na(df.majors[,i])] <- geomean.v[i] 
}
@

\noindent One then transform the dataset using the CLR transformation which is a mapping  $\mathbb{S}^D -> \mathbb{U}^D$ :
\begin{align}
\mathbf{z} = clr(\mathbf{x} = [log(x_1/g(x)),..,log(x_D/g(x))]
\end{align}
Where $\mathbb{U}^D$ is an hyperplane of $\mathbb{R}^D$ defined as : 
\begin{align*}
U^D = \Big\{[u_1,..,u_D] : \sum_{i=1}^D = 0  \Big\}
\end{align*}

<<echo=TRUE, cache=FALSE>>=
clr.df <- clr(df.majors)
clr.df.nafree <- clr(df.majors.nafree)
clr.df.nafree2 <- clr(df.majors2.nafree)
@

\section{Outlier Detection}

<<>>=
# outlier detection on the dataframe where missing values entries are replaced with column mean : 
ilrdf.mcd <- covMcd(clr2ilr(clr.df))
tolEllipsePlot(clr.df,classic = T,m.cov = clrdf.mcd)
1-length(ilrdf.mcd$best)/nrow(clr.df) # 44 % of outliers in this approach

# outlier detection on the dataframe where missing values are not taken into account
ilrdf.nafree.mcd <- covMcd(clr2ilr(clr.df.nafree))
plot(ilrdf.nafree.mcd,which = c("distance"),classic = TRUE) 
1-length(ilrdf.nafree.mcd$best)/nrow(clr.df.nafree) # 43 % of outliers in this approach

@

Is this proportion of outliers so high a particularity of the rock ? No, one sees that with the dataframe GeoPT46, again we flag almost 43\% of observations as outliers, this is too much for practical reasons. 
cov.mcd has a default parameter alpha which is the proportion of the total sample size to take a subset of through the relationship :
\begin{align}
\alpha \times n = h
\end{align}
We can take alpha = 0.80 so this means the breakdown value of our estimator would be 20 \%.
<<>>=
# A. outlier detection on the dataframe where missing values entries are replaced with column mean : 
ilrdf.mcd <- covMcd(clr2ilr(clr.df),alpha = .80)

plot(ilrdf.mcd,which = c("distance"),classic = TRUE) 
1-length(ilrdf.mcd$best)/nrow(clr.df) # 18 % of outliers in this approach
out <- c(2,7,35,28,42,51,52,35,47,73,81,84,78,45)
out.free <- clr.df[-out,]
# B. outlier detection on the dataframe where missing values are not taken into account
ilrdf.na.free.mcd <- covMcd(clr2ilr(clr.df.nafree),alpha = .80)
plot(ilrdf.nafree.mcd,which = c("distance"),classic = TRUE) 

1-length(ilrdf.nafree.mcd$best)/nrow(clr.df.nafree) ## 22 % of outliers

# We conduct SVD decomposition on the dataset without the 10 outliers (non robust method).

<<>>=
threshold <- sqrt(qchisq(p = 0.975, df = ncol(clr.df.nafree)))
outliers <-  which(ilrdf.nafree.mcd$mah >= threshold) # gives the 
#row numbers of outliers
out <- c(1,10,13,26,33,34,41,62,70,71)
nafree.out.free <- clr.df.nafree[-out,]

@

Conduct PCA on the dataset without missing values and without outliers.


<<>>=
ilrdf2.mcd <- covMcd(clr2ilr(clr.df.nafree2),alpha = .8)
plot(ilrdf2.mcd,which = c("distance"),classic = TRUE) 
1-length(ilrdf2.mcd$best)/nrow(clr.df.nafree2) # 17 % outliers in this approach, on a second dataset. 
@


Then principal component analysis is conducted.
Z denotes the mean-centered data matrix X :  
\begin{align*}
z_{ij} = x_{ij} - \mu_j
\end{align*}
Where $\mu_j$ denotes the arithmetic mean of the j-th column. Recall that here using the arithmetic mean is justified because X now lives in a subspace of $\mathbb{R}^D$ which is no longer constrained by the unit sum.

Perform Singular Value Decomposition on clr.df : 
\begin{align}
Z = UDW^T  = (UD)W^T = Z^{*}W^T,Z \in \mathbb{R}^{n\times(d-1)} U \in \mathbb{R}^{n\times p}, D \in \mathbb{R}^{p\times p}, W \in \mathbb{R}^{(d-1)\times p}
\end{align}
$Z^*$ denotes the projection of the mean-centered data matrix on a space of dimension $p$. Hopefully, $Z^*$ contains enough meaningful information about $Z$ while having a much lower number of dimensions. To evaluate how good $Z^*$ approximates $Z$, one looks at the proportion of variance explained by each of the components and more specifically, the cumulative proportion of variance explained by each of the components when these components are ranked from most to less important.



<<echo=TRUE, cache=FALSE>>=

# pca.clr <- prcomp(out.free[-c(33,72),],scale = T)
pca.clr <- prcomp(out.free,scale = T)

summary(pca.clr)
# repeat for nafree df
pca.clr.nafree <- prcomp(nafree.out.free,scale = T,rank. = ncol(clr.df)-1 )
summary(pca.clr.nafree)

@
The importance of a component, in terms of proportion of variance explained, is directly related to the D matrix whose eigenvalues squared are directly related to the proportion of variance explained through the relationship : 
\begin{align} \label{variancepca}
\lambda_i = d_i^2/(n-1)
\end{align}
Where $d_i$ denotes the i-th diagonal element in the square matrix D of the singular values and n is the number of principal components which is equal to the dimensionality of the original data matrix X.

Now, one looks at the rank-two approximation of Z (as the biplot does) :
<<>>=
# autoplot
autoplot(pca.clr,loadings=T,loadings.label=T)+theme_bw()
autoplot(pca.clr.nafree,loadings=T,loadings.label=T)+theme_bw()

# manually extracting the rank 2 approximation of Z
Z.approx <- data.frame(pca.clr$x[,c(1,2)])
colnames(Z.approx) = c("PC1","PC2")
Z.approx <- Z.approx %>% mutate(outlier="no")
# project outlying points in the first 2 PC space :
outliers.proj <- scale(clr.df[out,],pca.clr$center,pca.clr$scale) %*% t(pca.clr$rotation)
predict(pca.clr,clr.df[out])
outliers.coord <- data.frame(outliers.proj[,1:2])
names(outliers.coord) <- c("PC1","PC2")
outliers.coord <- outliers.coord %>% mutate(outlier="yes")
df.outfree <- rbind(Z.approx,outliers.coord)
ggplot(aes(x=PC1,y=PC2),data=df.outfree)+theme_bw()+geom_point(aes(colour=outlier))
@
Very strange thing going on here. The outliers does not appear to be outlying at all.
Observations 33 and 73 are also very strange. 

<<>>=
# manually extracting the rank 2 approximation of Z
Z.approx <- data.frame(pca.clr.nafree$x[,c(1,2)])
colnames(Z.approx) = c("PC1","PC2")
Z.approx <- Z.approx %>% mutate(outlier="no")
# project outlying points in the first 2 PC space :
outliers.proj <- scale(clr.df.nafree[out,],pca.clr$center,pca.clr$scale) %*% t(pca.clr$rotation)
predict(pca.clr,clr.df[out])
outliers.coord <- data.frame(outliers.proj[,1:2])
names(outliers.coord) <- c("PC1","PC2")
outliers.coord <- outliers.coord %>% mutate(outlier="yes")
df.outfree <- rbind(Z.approx,outliers.coord)
ggplot(aes(x=PC1,y=PC2),data=df.outfree)+theme_bw()+geom_point(aes(colour=outlier))

@





This approach has a pitfall. The estimation of the PC is itself not robust. Robcompositions package provides a robust PCA estimation method :
\begin{quote}
The compositional data set is expressed in isometric logratio coordinates. Afterwards, robust principal component analysis is performed. Resulting loadings and scores are back-transformed to the clr space where the compositional biplot can be shown. CITE ROBCOMP R package
\end{quote}
<<>>=
rob.pca.clr <- pcaCoDa(df.majors)
summary(rob.pca.clr)

@
One sees that the first two PC's found by using the robust method explains (89 \%) whereas in the classical method, the first two PC's explained only 76 \%. The outlier detection is repeated in the first 2 robust PC's subspace.
<<>>=
Z.approx.rob <- data.frame(rob.pca.clr$scores[,c(1,2)])
colnames(Z.approx.rob) = c("PC1","PC2")
ggplot(aes(x=PC1,y=PC2),data=Z.approx.rob)+theme_bw()+geom_point()
@
At first glance, there seem to be less outliers in the robust first two PC's space. This is checked by using diagnostic plots : 
<<>>=
Z2.rob.mcd <- covMcd(Z.approx.rob,)
tolEllipsePlot(Z.approx.rob,classic = T)
plot(Z2.rob.mcd,which = c("distance"),classic = TRUE)
Z2.rob.outfree <- Z.approx.rob[Z2.rob.mcd$best,]
ggplot(aes(x=PC1,y=PC2),data=Z2.rob.outfree)+theme_bw()+geom_point()+stat_ellipse()
@
There seems to be no association between PC1 and PC2, which makes much more sense. Eventually, an estimation for rock 1 composition is
<<>>=
df.majors.out.free <- df.majors[Z2.rob.mcd$best,]
df.majors.out.free.clr <- data.frame(clr(df.majors.out.free)) 
clr.mean <- colMeans(df.majors.out.free.clr)
clr.cov <- Cov(df.majors.out.free.clr)

mean.estimate <- as.vector(clrInv(clr.mean))
cov.estimate <- clrInv(as.matrix(clr.cov$cov))
cov.estimate
@
How does it compare with the naive estimates of GeoPT ? 
<<>>=
mean.naive <- colMeans(clo(df.majors))
od <- outCoDa(df.majors, quantile = 0.975, method = "robust", alpha = 0.9, coda = TRUE)
df.major.wo.outliers <- df.majors[od$outlierIndex,]
mean.naive.wo.outlier <- colMeans(clo(df.major.wo.outliers))
@
Not much difference for SiO2 \emph{but} TiO2 concentration is twice as high in the naive way, 5 times higher in the naive way when removing outliers. MnO concentration is 10 times higher in the naive way. 
<<>>=
df <- data.frame(mean.estimate,mean.naive,mean.naive.wo.outlier)
df
df$element <- row.names(df)
df.plot <- df %>% pivot_longer(cols=starts_with("mean"))
df.plot
df.plot$group <- ifelse(df.plot$element %in% c("SiO2","Al2O3"),yes = "large","small")
ggplot(aes(x=element,y=value),data = df.plot)+geom_col(aes(fill=name),position = "dodge")+theme_bw()+facet_grid(group~.,scales = "free")+scale_color_viridis_d()
@
Preliminary conclusion : naive without outlier downplay the concentration of elements present in large quantities and blow the concentration of elements present in small quantities.




\end{document}
